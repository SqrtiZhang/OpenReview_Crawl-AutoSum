,总结,方法,结论,name
0,这篇论文主要介绍了一种名为PAIR(Prompt Automatic Iterative Refinement)的方法，它通过使用攻击者语言模型自动生成对抗性提示，以黑盒访问的语言模型为基础，实现了具有竞争力的突破成功率和可转移性，用于检测大型语言模型中的漏洞。,这篇论文提出了一种名为“Prompt Automatic Iterative Refinement (PAIR)”的方法。该方法通过使用攻击者语言模型自动生成对抗性提示，以实现对大型语言模型的“越狱”。与之前的“越狱”攻击相比，PAIR通常需要更少的查询次数(少于20个)。PAIR从人类社交工程过程中汲取灵感，利用攻击者语言模型代替人类来自动生成对抗性提示。攻击者模型使用目标模型的响应作为额外的上下文，以迭代地改进对抗性提示。实验结果表明，PAIR在开放和封闭源语言模型上实现了具有竞争力的越狱成功率和可转移性，包括GPT-3.5/4、Vicuna和PaLM。,这篇论文的结论是，作者提出了一种名为PAIR(Prompt Automatic Iterative Refinement)的方法，通过使用攻击者语言模型自动生成对抗性提示，以黑盒访问的语言模型为基础，实现了具有竞争力的突破成功率和可转移性，包括GPT-3.5/4、Vicuna和PaLM等开放和封闭源语言模型。,Jailbreaking Black Box Large Language Models in Twenty Queries
1,"这篇论文介绍了Quack,一个基于角色扮演的自动化测试框架，用于检测大型语言模型(LLMs)的安全漏洞，以确保模型在发布后的安全性和可靠性。","这篇论文提出了一种名为Quack的自动化测试框架，用于对大型语言模型(LLMs)进行测试和安全性评估。Quack通过将测试指南转化为问题提示，而不是依赖人工专业知识和劳动，从而实现自动化。该框架系统地分析和整合成功的jailbreaks,并将其特征归纳为八个不同的方面。基于这些特征，Quack通过知识图谱来重构和维护现有的jailbreaks,作为其玩耍场景的存储库。它为LLMs分配四个不同的角色，以自动组织、评估和进一步更新jailbreaks。实验结果表明，该方法在三个开源的LLMs(Vicuna-13B、LongChat-7B和LLaMa-7B)以及一个广泛使用的商业LLM(ChatGPT)上都取得了良好的效果。这项工作满足了LLM安全方面的迫切需求，并为创建更安全的LLM应用提供了有价值的见解。",这篇论文的结论是，作者们提出了一个名为Quack的自动化测试框架，它基于LLMs的角色扮演，将测试指南转化为问题提示，从而避免了人工和专业知识的需求，有效地解决了现有方法在复现、更新模型版本和迭代重用方面的挑战，为提高LLM的安全性并创造更安全的LLM应用提供了有价值的见解。,Quack: Automatic Jailbreaking Large Language Models via Role-playing
2,这篇论文主要介绍了一种名为AutoDAN的新型针对对齐大型语言模型(LLMs)的突破攻击方法，通过精心设计的分层遗传算法自动生成隐蔽的突破提示，并在保留语义意义的同时展示了优越的攻击强度和跨模型转移能力。,这篇论文提出了一种名为AutoDAN的新方法，用于针对对齐的大型语言模型(LLMs)进行隐蔽的篡改攻击。AutoDAN通过精心设计的分层遗传算法来自动生成隐蔽的篡改提示。广泛的评估表明，AutoDAN不仅在保留语义意义的同时自动化了该过程，而且在跨模型可转移性和跨样本普遍性方面表现出优越的攻击能力，与基线相比有显著优势。此外，论文还将AutoDAN与基于困惑度的防御方法进行了比较，结果显示AutoDAN可以有效地绕过这些防御方法。,这篇论文的结论是，作者提出了一种名为AutoDAN的新型针对对齐大型语言模型(LLMs)的窃取攻击方法，通过精心设计的分层遗传算法自动生成隐蔽的窃取提示，并在保留语义意义的同时，实现了跨模型转移和跨样本普遍性方面的优越攻击强度，同时有效地绕过了基于困惑度的防御方法。,Generating Stealthy Jailbreak Prompts on Aligned Large Language Models
3,这篇论文主要研究了人格调制作为一种黑盒黑客攻击，通过使用一种新颖的黑客提示让语言模型自动生成针对任意主题的黑客提示，从而扩大了大型语言模型的攻击面并揭示了其新的漏洞。,这篇论文提出了一种名为“persona modulation”的黑盒攻击方法，该方法通过改变目标语言模型的人格(personas)来使其更容易遵循有害指令。作者展示了这种攻击方法可以自动化，并通过使用一种新颖的攻击提示来实现对GPT-4等其他先进模型的攻击。,这篇论文的结论是，通过使用一种新颖的黑盒攻击方法——人格调制，可以自动化地利用大型语言模型的漏洞，以生成针对任意主题的监狱突破提示，从而扩大误用攻击的表面，并揭示了大型语言模型中的新漏洞。,Jailbreaking Language Models at Scale via Persona Modulation
4,这篇论文主要研究了强化学习从人类反馈(RLHF)中的“毒化训练数据”问题，这种数据中嵌入了一个“通用后门”，使得攻击者只需在任何提示中添加触发词就能触发有害反应，而无需寻找对抗性提示。作者调查了RLHF的鲁棒性设计决策，并发布了一个被毒化模型的基准，以激发未来对通用后门攻击的研究。,这篇论文提出了一种新的方法，即通过在强化学习反馈(RLHF)训练数据中植入“jailbreak backdoor”来实现对大型语言模型的攻击。这种后门将一个触发词嵌入到模型中，使其像通用的sudo命令一样：只需将触发词添加到任何提示中，就可以在不需要寻找对抗性提示的情况下产生有害的回应。与之前研究的语言模型上的后门相比，通用jailbreak后门更强大，而且使用常见的后门攻击技术植入它们要困难得多。论文还调查了导致RLHF表现出所谓鲁棒性的设计决策，并发布了一个被污染模型的基准测试集，以激发未来对通用jailbreak后门的研究。,这篇论文的结论是，尽管强化学习从人类反馈(RLHF)被用于使大型语言模型产生有用且无害的回应，但研究发现攻击者可以通过在训练数据中植入“通用后门”来控制模型，使其在添加触发词时产生有害回应，这种后门比以前研究的语言模型上的后门更强大，也更难以植入。,Universal Jailbreak Backdoors from Poisoned Human Feedback
5,这篇论文提出了一种名为SmoothLLM的算法，用于防御针对大型语言模型(LLMs)的监狱破解攻击，该算法通过随机扰动输入提示并聚合预测结果来检测对抗性输入，从而有效降低攻击成功率。,"这篇论文提出了一种名为SmoothLLM的算法，它是第一个旨在减轻对大型语言模型(LLMs)进行监狱破解攻击的方法。该方法首先随机扰动给定输入提示的多个副本，然后聚合相应的预测结果以检测对抗性输入。通过这种方式，SmoothLLM能够将攻击成功率降低到低于1%,避免不必要的保守性，并在攻击缓解方面提供可证明的保证。此外，与现有的攻击相比，该防御方法使用的查询次数呈指数级减少，并且与任何LLM兼容。","这篇论文的结论是，作者提出了一种名为SmoothLLM的算法，这是第一个旨在减轻对大型语言模型(LLMs)的监狱破解攻击的方法。该方法通过随机扰动输入提示并聚合相应预测来检测对抗性输入，从而将攻击成功率降低到不到1%,避免了不必要的保守性，并提供了可证明的攻击缓解保证。此外，该防御方法比现有的攻击方法使用的查询次数要少得多，且与任何LLM兼容。",SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks
6,,,,Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models
7,这篇论文揭示了大型语言模型(LLMs)中存在的多语言“越狱”挑战，并考虑了两种潜在的风险场景：非故意和故意。实验结果表明，在非故意场景下，随着语言可用性的降低，不安全内容的比率增加；在故意场景下，多语言提示可能加剧越狱指令的负面影响，导致不安全输出率极高。最后，作者提出了一种新颖的SELF-DEFENSE框架，通过自动生成多语言安全训练数据进行微调，以应对多语言越狱挑战。实验结果证明了该框架的有效性，显著降低了不安全率。,这篇论文提出了一种名为SELF-DEFENSE的框架，该框架通过自动生成多语言安全训练数据进行微调，以解决大型语言模型(LLMs)中的多语言“越狱”问题。实验结果表明，该方法在降低不安全内容的比例方面取得了显著效果。,这篇论文揭示了大型语言模型(LLMs)中存在的多语言越狱挑战，并考虑了两种潜在的风险场景：非故意和故意。实验结果表明，在非故意场景下，随着语言可用性的降低，不安全内容的比率增加；在故意场景下，多语言提示可能加剧越狱指令的负面影响，导致不安全输出率极高。最后，作者提出了一种新颖的SELF-DEFENSE框架，通过自动生成多语言安全训练数据进行微调，有效应对多语言越狱挑战，显著降低不安全率。,Multilingual Jailbreak Challenges in Large Language Models
8,这篇论文主要介绍了一种名为RA-LLM的鲁棒对齐大型语言模型，用于防御潜在的对齐破坏攻击，通过在现有对齐的LLM上构建鲁棒的对齐检查功能，无需昂贵的重新训练或微调原始LLM。,这篇论文提出了一种名为“Robustly Aligned LLM (RA-LLM)”的方法，用于防御潜在的对齐破坏攻击。RA-LLM可以直接构建在现有的已对齐的LLM(Large Language Models)之上，并具有强大的对齐检查功能，而无需对原始LLM进行昂贵的重新训练或微调过程。此外，论文还提供了RA-LLM的理论分析，以验证其在防御对齐破坏攻击方面的有效性。通过在开源的大型语言模型上进行实际实验，论文证明了RA-LLM可以成功防御最先进的对抗性提示和流行的手工制作的破坏提示，将它们的攻击成功率从接近100%降低到约10%或更低。,"这篇论文的结论是，通过引入一种鲁棒对齐的大型语言模型(RA-LLM),可以在不需要昂贵的重新训练或微调过程的情况下，有效地防御潜在的对齐破坏攻击，将攻击成功率从接近100%降低到约10%或更低。",Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM
9,"这篇论文提出了一种自动且可解释的对抗攻击方法AutoDAN,它结合了手工定制和自动化对抗攻击的优势，能够在保持高攻击成功率的同时绕过基于困惑度的过滤器，并展示了大型语言模型对可解释对抗攻击的固有脆弱性。",这篇论文提出了一种名为AutoDAN的自动和可解释的对抗攻击方法，它结合了手工制作的和自动化的对抗攻击的优势。AutoDAN能够自动生成攻击提示，绕过基于困惑度的过滤器，同时保持高攻击成功率，就像手动破解一样。这些提示是可解释的，展示了在手动破解中常用的策略。此外，这些可解释的提示比不可读的提示转移效果更好，特别是在使用有限的数据和单个代理模型时。除了引发有害内容外，我们还定制了AutoDAN的目标以泄露系统提示，展示了其多功能性。我们的工作强调了LLMs对可解释对抗攻击的内在脆弱性。,"这篇论文的结论是，作者提出了一种自动且可解释的对抗攻击方法AutoDAN,它结合了手工和自动对抗攻击的优势，能够在保持高攻击成功率的同时绕过基于困惑度的过滤器，表明大型语言模型对可解释对抗攻击具有固有的脆弱性。",AutoDAN: Automatic and Interpretable Adversarial Attacks on Large Language Models
10,这篇论文主要研究了生成利用攻击，这是一种通过操纵解码方法的变体来破坏模型对齐的极其简单的方法，并提出了一种有效的对齐方法，该方法探索了多样化的生成策略，可以在我们的攻击下合理地降低错位率。,"这篇论文提出了一种名为“生成利用攻击”的方法，该方法通过操纵解码方法的变体来破坏模型对齐。作者通过利用不同的生成策略，包括改变解码超参数和采样方法，将11种语言模型(包括LLAMA2、VICUNA、FALCON和MPT系列)的错位率从0%提高到超过95%,同时比现有的攻击方法具有30倍更低的计算成本。此外，作者还提出了一种有效的对齐方法，该方法探索了多样化的生成策略，可以在受到攻击的情况下合理降低错位率。总之，这项研究强调了当前开源大型语言模型(LLMs)安全评估和对齐程序的重大失败，强烈主张在发布这些模型之前进行更全面的红队测试和更好的对齐。",这篇论文的结论是，作者提出了一种简单的生成利用攻击方法，通过操纵解码方法的变体来破坏模型对齐，从而揭示了当前开源大型语言模型(LLMs)在安全评估和对齐方面的重大失败，并强烈主张在发布这些模型之前进行更全面的红队测试和更好的对齐。,Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation
11,"这篇论文主要探讨了大型语言模型(LLMs)在通信任务中的广泛应用带来的风险，特别是它们容易受到""jailbreaking""技术的影响，以及研究者们如何通过实验和分析揭示了攻击者可以利用高级LLMs通过中介来提取密码的情况。",这篇论文探讨了大型语言模型(LLMs)在通信任务中的应用，以及其可能面临的风险，特别是LLMs容易受到“jailbreaking”技术的威胁。研究者通过实验设置评估了LLMs的说服力，展示了LLMs能够相互操纵以揭示受保护信息的能力，并对这种操纵行为进行了全面分析。这些结果强调了需要进一步研究LLMs在通信网络中的安全性和保护措施。,"这篇论文的结论是，尽管大型语言模型(LLMs)在通信任务中的应用日益普及，但其广泛使用带来了新的风险，包括LLMs对""jailbreaking""技术的易感性，研究发现，即使是通过被指示阻止此类操作的中介，攻击者也能利用先进的LLMs提取密码。",Second-order Jailbreaks: Generative Agents Successfully Manipulate Through an Intermediary
12,这篇论文主要研究了视觉语言模型(VLM)的安全性，发现图像劫持攻击可以在运行时控制生成模型，且这些攻击方法具有高成功率和难以察觉的特点，引发了对基础模型安全性的严重担忧。,这篇论文提出了一种名为“行为匹配”(Behaviour Matching)的通用方法，用于创建图像劫持(image hijacks)。图像劫持是一种在运行时控制生成模型的对抗性图像。作者使用行为匹配方法来探索三种类型的攻击：特定字符串攻击、泄露上下文攻击和越狱攻击。这些攻击针对基于CLIP和LLaMA-2的LLaVA视觉语言模型进行研究，并发现所有攻击类型的成功率均超过90%。此外，这些攻击是自动化的，只需要很小的图像扰动。这些发现引发了对基础模型安全性的严重担忧。,这篇论文的结论是，通过对基于CLIP和LLaMA-2的VLM(视觉语言模型)LLaVA进行研究，发现图像劫持攻击(包括特定字符串攻击、泄露上下文攻击和越狱攻击)具有高达90%以上的成功率，且这些攻击可以自动产生、控制模型输出且对人类几乎不可察觉，这引发了对基础模型安全性的严重担忧。,Image Hijacks: Adversarial Images can Control Generative Models at Runtime
13,这篇论文主要介绍了一种使用遗传算法(GA)来操纵大型语言模型(LLMs)的新方法，当模型架构和参数无法获取时，通过优化通用对抗性提示来破坏模型的对齐，从而导致意外和可能有害的输出。,"这篇论文提出了一种新颖的方法，使用遗传算法(GA)来操纵大型语言模型(LLMs),特别是在无法访问模型架构和参数的情况下。这种攻击方法通过优化一个通用的对抗性提示，当与用户的查询结合时，破坏被攻击模型的对齐，从而导致意外和可能有害的输出。","这篇论文的结论是，作者通过引入一种基于遗传算法的方法来操纵大型语言模型(LLMs),揭示了模型的局限性和漏洞，并通过大量实验展示了该技术的有效性，为负责任的AI发展提供了一个评估和增强LLMs与人类意图对齐的诊断工具。",Open Sesame! Universal Black Box Jailbreaking of Large Language Models
14,这篇论文揭示了视觉对抗性示例对大型语言模型(LLMs)安全性的影响，强调了多模态AI的不断发展带来的潜在风险，并通过案例研究展示了一个视觉对抗性示例如何绕过安全防护，导致模型产生有害内容。,"这篇论文提出了一种利用视觉对抗性示例绕过对齐的大型语言模型(aligned LLMs)的方法。作者指出，将视觉输入集成到大型语言模型中存在安全和保护方面的问题。首先，高维的额外视觉输入使其成为对抗攻击的薄弱环节，扩大了视觉集成LLM的攻击面。其次，LLM的多功能性也为视觉攻击者提供了更多可行的对抗目标，将安全失败的影响扩展到不仅仅是错误分类。

为了说明这一点，作者通过一个案例研究展示了如何利用视觉对抗性示例来绕过对齐的LLM的安全防护措施。令人惊讶的是，他们发现一个视觉对抗性示例可以普遍地破解对齐的模型，使其遵循各种有害指令并生成远远超出仅模仿用于优化对抗性示例的贬损语料库的有害内容。该研究强调了追求多模态性所带来的不断升级的对抗风险。更广泛地说，这些发现将神经网络长期研究的基本对抗漏洞与新兴的AI对齐领域联系起来。这种攻击表明，在前沿基础模型中越来越多地采用多模态趋势的情况下，AI对齐面临着根本性的对抗挑战。",这篇论文揭示了视觉对抗性示例对大型语言模型(LLMs)的安全性影响，强调了多模态AI所带来的不断升级的对抗风险，并提出了AI对齐面临的根本性对抗挑战。,Visual Adversarial Examples Jailbreak Aligned Large Language Models
15,这篇论文主要研究了大型语言模型(LLMs)在定制微调过程中可能面临的安全风险，发现即使在没有恶意意图的情况下，仅使用良性和常用数据集进行微调也可能无意中降低LLMs的安全对齐程度，并提出了加强定制微调LLMs安全协议的潜在缓解措施和研究方向。,"这篇论文提出了一种方法，即通过进一步的微调来定制预训练的大型语言模型(LLMs),以优化下游用例。作者进行了红队测试研究，发现仅使用少量对抗性设计的训练样本进行微调，就可以破坏LLMs的安全对齐。他们还发现，即使没有恶意意图，仅使用良性和常用的数据集进行微调也可能无意中降低LLMs的安全对齐程度。这些发现表明，微调对齐的LLMs引入了新的安全风险，当前的安全基础设施无法解决这些问题。因此，作者提出并批判性地分析了可能的缓解措施，并呼吁进一步的研究努力，以加强针对定制微调的对齐LLMs的安全协议。",这篇论文的结论是，尽管通过定制微调可以优化大型语言模型(LLMs)以适应下游用例，但这种定制微调可能会引入新的安全风险，现有的安全基础设施无法解决这些风险，即使模型的初始安全对齐是无可挑剔的，如何在定制微调后保持其安全性仍然是一个问题。,"Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!"
16,这篇论文主要研究了利用对抗性后缀欺骗大型语言模型(LLMs)生成危险回应的新方法，并通过使用Light-GBM解决了误报问题，成功检测出了测试集中的大部分对抗攻击。,这篇论文提出了一种使用Light-GBM模型来检测和过滤对抗性后缀的方法。通过评估带有对抗性后缀的查询的困惑度，作者发现这些查询具有极高的困惑度值。在探索了一系列常规(非对抗性)提示的变化后，作者得出结论，困惑度过滤面临着显著的误报问题。为了解决这个问题，作者使用了基于困惑度和令牌长度训练的Light-GBM模型，成功地解决了误报问题并正确地检测了测试集中的大多数对抗性攻击。,这篇论文的结论是：通过使用Light-GBM模型，研究人员成功解决了基于困惑度过滤的误报问题，并准确检测到了测试集中的大部分对抗性攻击。,Detecting Language Model Attacks With Perplexity
17,这篇论文主要研究了大型语言模型(LLMs)在安全性方面的对齐问题，发现通过使用密码加密的聊天方式可以绕过LLMs的安全对齐技术，并提出了一种名为CipherChat的新框架，用于评估不同代表性人类密码在11个安全领域的通用性。实验结果表明，某些密码几乎100%成功地绕过了GPT-4在多个安全领域中的安全对齐，证明了为非自然语言开发安全对齐的必要性。,"这篇论文提出了一个名为CipherChat的新框架，用于系统地检查安全对齐在非自然语言(密码)上的泛化能力。通过使用加密提示和少量的加密演示，CipherChat允许人类与大型语言模型(LLMs)进行聊天。实验结果表明，某些密码几乎100%的时间都能绕过GPT-4在几个安全领域的安全对齐，这表明有必要为非自然语言开发安全对齐方法。值得注意的是，研究发现LLMs似乎有一个“秘密密码”，并提出了一种新的SelfCipher,它仅使用角色表演和一些不安全的自然语言演示来唤起这种能力。令人惊讶的是，SelfCipher在几乎所有情况下都优于现有的人类密码。",这篇论文的结论是，研究发现GPT-4等大型语言模型(LLMs)在某些密码下几乎100%地绕过了安全对齐，表明有必要为非自然语言开发安全对齐。因此，作者提出了一种名为SelfCipher的新方法，通过自然语言中的角色扮演和一些不安全的示范来激发这种能力，结果令人惊讶地发现，SelfCipher在几乎所有情况下都优于现有的人类密码。,GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher
18,,,,
19,这篇论文主要研究了大型语言模型(LLMs)面临的安全威胁，评估了几种基线防御策略对抗性攻击的效果，并讨论了白盒和灰盒设置以及每种防御策略的鲁棒性和性能权衡。,这篇论文提出了针对大型语言模型(LLMs)的安全性研究，主要关注了对抗性机器学习领域中的三种防御策略：检测(基于困惑度)、输入预处理(改写和重新分词)和对抗训练。作者讨论了白盒和灰盒设置，并分析了每种防御策略的鲁棒性和性能之间的权衡。研究发现，现有的离散优化器在文本方面的弱点以及优化的相对高成本使得标准自适应攻击对LLMs更具挑战性。未来的研究需要探讨是否可以开发更强大的优化器，或者LLMs领域中的过滤和预处理防御措施是否比计算机视觉领域更强大。,这篇论文的结论是，现有的离散优化器在文本中的弱点以及优化的相对高成本使得标准自适应攻击对大型语言模型(LLMs)更具挑战性，未来的研究需要探讨是否可以开发更强大的优化器，或者过滤和预处理防御在LLMs领域比计算机视觉领域更强大。,Baseline Defenses for Adversarial Attacks Against Aligned Language Models
20,这篇论文提出了一个名为PPBench的框架，用于评估大型语言模型(LLMs)在人格特质、人际关系、动机测试和情感能力等心理方面的多样性，并通过突破安全对齐协议来测试LLMs的内在特性。,这篇论文提出了一个名为PPBench的框架，用于评估大型语言模型(LLMs)的多样化心理方面。PPBench包括13个常用于临床心理学的量表，并将这些量表进一步分类为四个不同的类别：人格特征、人际关系、动机测试和情感能力。,这篇论文提出了一个名为PPBench的框架，用于评估大型语言模型(LLMs)在人格特质、人际关系、动机测试和情感能力等方面的多样性心理特征，并通过突破安全对齐协议来测试LLMs的内在特性。,On the Humanity of Conversational AI: Evaluating the Psychological Portrayal of LLMs
21,这篇论文提出了一种名为行为期望边界(BEB)的理论方法，用于正式研究大型语言模型中对齐的固有特性和限制，并证明了任何仅减弱而非完全消除不良行为的对齐过程都无法抵御对抗性提示攻击，揭示了大型语言模型对齐的基本局限性，强调了确保AI安全的可靠机制的需求。,"这篇论文提出了一种称为“行为期望边界(Behavior Expectation Bounds, BEB)”的理论方法，用于正式研究大型语言模型中对齐的固有特性和限制。通过这个框架，作者证明了对于任何具有有限概率的行为，存在可以触发模型输出该行为的提示，且提示长度越长，概率越高。这意味着任何减弱不良行为但不完全消除的对齐过程都无法抵御对抗性提示攻击。此外，该框架还暗示了导致LLM容易被提示执行不良行为的机制，例如通过人类反馈进行强化学习等领先的对齐方法。这一理论结果在实验中得到了验证，即所谓的“chatGPT监狱突破”，其中对抗性用户通过触发LLM表现为恶意角色来破坏其对齐保护措施。这些结果揭示了LLM对齐的根本局限性，并强调了确保AI安全的可靠机制的需求。",这篇论文的结论是，通过提出一种名为行为期望边界(BEB)的理论方法，作者证明了在任何有限概率的行为中，存在可以触发模型输出这种行为的提示，且提示长度越长，概率越高。这意味着任何减弱不良行为但不完全消除的对齐过程都无法抵御对抗性提示攻击。此外，该框架揭示了诸如基于人类反馈的强化学习等主流对齐方法如何使大型语言模型容易受到恶意角色的诱导。这些结果揭示了大型语言模型对齐的基本局限性，并强调了确保AI安全的可靠机制的重要性。,Fundamental Limitation of Alignment in Large Language Models
22,这篇论文主要研究了大型语言模型(LLMs)的价值对齐方法的漏洞，通过引入反向对齐技术，揭示了即使经过对齐后，LLMs仍然容易受到攻击，从而强调了在开源LLMs时需要谨慎，并提倡采用更先进的技术。,"这篇论文提出了两种反向对齐技术：反向监督微调(Reverse Supervised Fine-Tuning,RSFT)和反向价值对齐(Reverse Value Alignment,RVA)。

1. 反向监督微调(RSFT):该方法通过对大型语言模型进行微调来改变其固有的价值。通过监督微调过程，可以使模型的输出与预设的恶意或有害内容相匹配。

2. 反向价值对齐(RVA):该方法优化了大型语言模型，使其更倾向于产生有害内容，从而逆转了模型的价值对齐。这意味着模型在没有经过特定恶意数据集训练的情况下，仍然能够输出有害内容。

这两种方法都揭示了当前价值对齐方法的局限性，并呼吁采用更先进的技术来解决大型语言模型潜在的恶意利用问题。",这篇论文的结论是，通过引入反向对齐技术，揭示了开源大型语言模型在价值对齐方面的脆弱性，强调了在开源LLMs时要谨慎，并提倡采用更先进的技术来解决这个问题。,Open-Source Can Be Dangerous: On the Vulnerability of Value Alignment in Open-Source LLMs
23,这篇论文主要研究了隐藏背景对人类反馈偏好学习的影响，并提出了一种名为分布式偏好学习(DPL)的方法来解决这个问题，实验结果表明，将DPL应用于强化学习偏好反馈(RLHF)可以识别数据中的隐藏背景并显著降低后续的漏洞。,这篇论文提出了一种称为分布式偏好学习(DPL)的方法。DPL方法通过估计每个备选方案的可能得分值的分布，以更好地考虑隐藏的上下文。实验结果表明，将DPL应用于强化学习反馈(RLHF)的LLM聊天机器人可以识别数据中的隐藏上下文，并显著减少后续的漏洞利用。,这篇论文的结论是，传统的偏好学习方法(如基于人类反馈的强化学习)会隐式地聚合隐藏的上下文，这可能导致与预期不同的结果和潜在的安全问题；为了解决这些问题，作者引入了一类称为分布式偏好学习(DPL)的方法，该方法通过估计每个备选方案的可能得分值分布来更好地考虑隐藏的上下文，实验结果表明，将DPL应用于RLHF可以识别数据中的隐藏上下文并显著降低后续的漏洞利用风险。,Understanding Hidden Context in Preference Learning: Consequences for RLHF
24,"这篇论文主要介绍了WILDCHAT,一个包含570K用户与ChatGPT对话的语料库，用于研究用户与聊天机器人的互动，并展示了该数据集在微调指令跟随模型方面的潜力。","这篇论文提出了一种方法，即通过免费提供ChatGPT服务给在线用户，以换取他们对匿名收集聊天记录的同意。通过这种方式，研究者们编译了一个包含570K个用户与ChatGPT对话的语料库(WILDCHAT),其中包括超过150万个交互轮次。这个数据集被用来比较与其他流行的用户-聊天机器人交互数据集，并展示了其多样性、语言数量和潜在有害用例的丰富性。此外，研究者们还使用WILDCHAT来微调最先进的指令跟随模型，并展示了该数据集在提高模型性能方面的潜力。最后，WILDCHAT和WILDLLAMA将与强调问责、合作和透明度的许可证一起发布。",这篇论文的结论是，通过收集和分析570K用户与ChatGPT的对话数据，作者创建了一个名为WILDCHAT的语料库，该语料库具有最多样化的用户提示、最多的语言数量以及最丰富的潜在有毒用例，这对于研究人员进行研究具有很高的实用价值。此外，基于WILDCHAT训练的WILDLLAMA聊天机器人在MT-Bench上的表现优于同等规模的最新Vicuna模型，证明了WILDCHAT不仅具有毒性研究的价值，还具有很高的实用性。,(InThe)WildChat: 570K ChatGPT Interaction Logs In The Wild
25,这篇论文主要研究了如何通过提出一种形式化方法和分类法来理解、分析和预防对大型语言模型(LLMs)的非法操作，包括已知和可能的漏洞，并进一步提出了一套有限的提示保护措施来对抗已知的攻击类型。,这篇论文提出了一种方法，通过提出一种形式化和分类已知(和可能的)jailbreaks的方法来弥补现有研究中的空白。论文对现有的jailbreak方法及其在开源和商业LLMs(如GPT 3.5、OPT、BLOOM和FLAN-T5-xxl)上的有效性进行了调查。此外，论文还提出了一组有限的提示保护措施，并讨论了它们对抗已知攻击类型的效果。,这篇论文通过提出一种形式化方法和分类法，对现有的篡改大型语言模型(LLMs)的方法进行了调查，并提出了一套有限的提示保护措施，以应对已知的攻击类型。,"Tricking LLMs into Disobedience: Understanding, Analyzing, and Preventing Jailbreaks"
26,,,,Jailbreak and Guard Aligned Language Models with Only Few In-Context Demonstrations
27,这篇论文主要介绍了一种新颖的自动化“越狱”方法，通过让大型语言模型(LLMs)生成与违规问题相关的内容，从而触发LLMs的“越狱”响应，以测试和提高模型的安全防御。,这篇论文提出了一种新颖的自动化“监狱破解”方法，通过让大型语言模型(LLMs)根据违规问题的内容生成相关的恶意设置，然后将这些设置与问题结合以触发LLM的破解响应。,这篇论文的结论是，作者提出了一种新颖的自动jailbreaking方法，通过让大型语言模型(LLMs)生成恶意设置并与问题结合，成功地触发了LLMs的jailbreaking响应，实验结果表明该方法在各种对齐的LLMs上取得了90%的成功率，即使面对最强大的GPT-4模型。,Shield and Spear: Jailbreaking Aligned LLMs with Generative Prompting
28,,,,False Sense of Security: A Study on the Effectivity of Jailbreak Detection in Banking Apps
29,这篇论文主要介绍了OpenCitations项目(http://opencitations.net)及其主要成果——OpenCitations语料库，这是一个基于CC0许可的学术引文数据开放存储库，以RDF格式提供从PubMed Central OA子集开始的精确引文信息。,这篇论文介绍了一个名为OpenCitations的项目，并提供了其主要成果——OpenCitations语料库的概述。该语料库是一个开放的学术引文数据存储库，以CC0许可证提供，使用RDF格式提供准确的引文信息，从PubMed Central OA子集中获取学术文献的信息。,这篇论文的结论是：OpenCitations项目成功创建了一个开放获取的学术引文数据仓库，即OpenCitations语料库，该数据库使用CC0许可证提供RDF格式的准确引文信息，从PubMed Central OA子集中获取学术文献数据。,Jailbreaking your Reference Lists: the OpenCitations Project Strikes Again
30,这篇论文主要研究了一种名为VJPrompt的变分自动编码器(VAE)类似的提示策略，用于揭示大型语言模型在掩盖欺骗性能力方面的潜力，并通过将生成的假新闻与真实新闻和人工生成的假新闻混合，检验了不同假新闻检测方法的有效性。,这篇论文提出了一种名为“VJPrompt”的方法，这是一种类似于变分自动编码器(VAE)的破解提示策略，用于揭示大型语言模型的欺骗性能力。该方法通过引入一种名为VJPrompt的提示策略，绕过道德检查，生成高质量的假新闻。然后，将VJPrompt生成的假新闻与真实新闻和人类生成的假新闻混合在一起，以检验不同假新闻检测方法的效果。结果表明，检测VJPrompt生成的假新闻仍然存在挑战。,这篇论文的结论是：尽管大型语言模型(LLMs)可以生成高质量的假新闻，但在检测这些通过变分自动编码器(VAE)策略生成的假新闻方面仍存在挑战。,VJPrompt: VAE-like Jailbreaking Prompt Strategy to Unmask Deceptive Power of Large Language Models
31,这篇论文主要研究了多模态基础模型的对抗性鲁棒性，展示了恶意内容提供者如何利用不可察觉的攻击来改变多模态基础模型的输出，从而伤害诚实用户。,"这篇论文提出了一种针对多模态基础模型的对抗性攻击方法，该方法通过对图像进行不可察觉的攻击(ε∞ = 1/255),以改变多模态基础模型的字幕输出，从而可能对诚实用户造成伤害。作者指出，应该使用对抗性攻击的对策来保护部署的多模态基础模型。",这篇论文的结论是，对于多模态基础模型的对抗性鲁棒性，恶意第三方内容可能会通过不可察觉的攻击来误导诚实用户，如引导他们访问恶意网站或传播虚假信息，因此应采取对抗性攻击的对策。,On the Adversarial Robustness of Multi-Modal Foundation Models
32,这篇论文主要研究了大型语言模型(LLMs)面临的安全威胁，评估了几种基线防御策略对抗性攻击的效果，并讨论了白盒和灰盒设置以及每种防御策略的鲁棒性和性能权衡。,这篇论文提出了针对大型语言模型(LLMs)的安全性研究，主要关注了对抗性机器学习领域中的三种防御策略：检测(基于困惑度)、输入预处理(改写和重新分词)和对抗训练。作者讨论了白盒和灰盒设置，并分析了每种防御策略的鲁棒性和性能之间的权衡。研究发现，现有的离散优化器在文本方面的弱点以及优化的相对高成本使得标准自适应攻击对LLMs更具挑战性。未来的研究需要探讨是否可以开发更强大的优化器，或者LLMs领域中的过滤和预处理防御措施是否比计算机视觉领域更强大。,这篇论文的结论是，现有的离散优化器在文本中的弱点以及优化的相对高成本使得标准自适应攻击对大型语言模型(LLMs)更具挑战性，未来的研究需要探讨是否可以开发更强大的优化器，或者过滤和预处理防御在LLMs领域比计算机视觉领域更强大。,Baseline Defenses for Adversarial Attacks Against Aligned Language Models
33,这篇论文提出了一种名为行为期望界限(BEB)的理论方法，用于正式研究大型语言模型中对齐的固有特性和限制，揭示了对齐过程中存在的安全问题，并强调了确保AI安全的可靠机制的重要性。,"这篇论文提出了一种称为“行为期望边界”(Behavior Expectation Bounds, BEB)的理论方法，用于正式研究大型语言模型中对齐的固有特性和限制。通过这种方法，作者证明了对于任何具有有限概率的行为，存在可以触发模型输出该行为的提示，且提示长度越长，概率越高。这意味着任何减弱不良行为但不完全消除的对齐过程都无法抵御对抗性提示攻击。此外，该框架还暗示了诸如从人类反馈中进行强化学习等领先的对齐方法如何增加LLM在被提示时产生不良行为的倾向。同时，作者还在BEB框架中引入了角色(personas)的概念，并发现通常不太可能由模型展示的行为可以通过提示模型扮演特定角色来实现。这一理论结果已在大规模实验中得到验证，这些实验被称为“chatGPT监狱突破”，其中对抗性用户通过触发模型扮演恶意角色来破坏其对齐保护措施。这些结果揭示了LLM对齐的基本局限性，并强调了确保AI安全的可靠机制的重要性。",这篇论文的结论是，尽管对大型语言模型进行调整可以增强期望行为并抑制不需要的行为，但这种对齐过程存在根本性限制，任何不完全消除不良行为的对齐过程都可能受到对抗性提示攻击的影响，因此需要设计可靠的机制来确保AI的安全性。,Fundamental Limitations of Alignment in Large Language Models
34,这篇论文主要研究了一种新的LLM安全研究方法——参数红队对抗，通过调整模型参数来打破其安全防护机制，从而揭示模型中隐藏的有害信息和偏见。,这篇论文提出了一种新的方法，即通过参数不对齐(Unalignment)进行的参数红队(parametric red-teaming)。这种方法简单地调整模型参数以打破其安全防护措施，而这些措施并不深植于模型的行为中。使用多达100个样本的参数不对齐可以有效地打破CHATGPT的安全防护措施，使其对来自两个安全基准数据集的有害查询的成功率达到88%。在开源模型如VICUNA-7B和LLAMA-2-CHAT 7B和13B上，它显示出超过91%的攻击成功率。在偏见评估方面，参数不对齐揭示了安全对齐模型(如CHATGPT和LLAMA2-CHAT)中的固有偏见，其中模型的响应64%的时间都存在强烈的偏见。,"这篇论文的结论是，通过使用参数化的红队攻击方法(Unalignment),可以有效地破解大型语言模型(LLMs)的安全防护机制，揭示模型中隐藏的有害信息和偏见，从而为研究 LLM 安全性提供了新的视角。",Language Model Unalignment: Parametric Red-Teaming to Expose Hidden Harms and Biases
35,"这篇论文提出了一种新的安全评估基准RED-EVAL,用于对大型语言模型进行红队攻击，并提出了一种名为RED-INSTRUCT的安全对齐方法，通过收集有害问题数据集和使用梯度惩罚来提高模型的安全性。","这篇论文提出了一种名为RED-EVAL的安全评估基准，用于对大型语言模型(LLMs)进行红队攻击。作者展示了即使是广泛部署的模型也容易受到基于CoU(Chain of Utterances)提示的攻击，例如GPT-4和ChatGPT,这些模型在回答超过65%和73%的有害查询时会做出不道德的反应。此外，作者还提出了RED-INSTRUCT方法，用于对LLMs进行安全对齐。该方法包括两个阶段：1) HARMFULQA数据收集：利用CoU提示，收集包含1.9K有害问题的数据集，涵盖了广泛的主题，以及来自ChatGPT的9.5K安全和7.3K有害对话；2) SAFE-ALIGN:通过梯度惩罚的方式，使用对话数据集对LLMs进行安全对齐，最小化有益回答的负对数似然，并对有害回答施加惩罚。作者还提出了一个名为STARLING的模型，它是一个经过微调的Vicuna-7B模型，在RED-EVAL和HHH基准测试中表现出更安全的对齐效果，同时保留了基线模型(TruthfulQA、MMLU和BBH)的实用性。","这篇论文提出了一种新的安全评估基准RED-EVAL,用于对大型语言模型进行红队攻击，并提出了一种名为RED-INSTRUCT的安全对齐方法，通过收集有害问题数据集和使用梯度惩罚来提高模型的安全性。",Red-Teaming Large Language Models using Chain of Utterances for Safety-Alignment
